{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " MVAE-BM TwitterHate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4FGwv4oz74e",
        "colab_type": "text"
      },
      "source": [
        "### Please use this notebook inside google colab environment. In Runtime select \"Run All\", and after the execution, it will show the accuracy result of MVAE-BM in TwitterHate Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0XE1D7tcywb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "110ce564-0e70-4476-a212-21a21f6a2792"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1UcsXn-hO7APQpDWBcgCwbFbduUePd6e3',\n",
        "                                    dest_path='./tweets.zip',\n",
        "                                    unzip=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l9q1EacZqpf",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = np.load(\"tweets.npy\",allow_pickle=True)\n",
        "labels = np.load(\"labels-tweets.npy\",allow_pickle=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train,labels, test_size=0.33, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "X_train = [' '.join(x) for x in X_train]\n",
        "X_test = [' '.join(x) for x in X_test]\n",
        "X_valid = [' '.join(x) for x in X_valid]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5jIsukR7SIo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af1079b7-1062-4b26-e2d6-a1bff0f2305f"
      },
      "source": [
        "#@title Bag of Words Encode\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from random import shuffle\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "trainX = vectorizer.fit_transform(X_train)\n",
        "validX = vectorizer.transform(X_valid)\n",
        "testX = vectorizer.transform(X_test)\n",
        "\n",
        "voc = vectorizer.vocabulary_\n",
        "\n",
        "\n",
        "indices = np.arange(trainX.shape[0]) #gets the number of rows \n",
        "shuffle(indices)\n",
        "\n",
        "\n",
        "print('train shape:',trainX.shape,\"test shape:\",testX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape: (10194, 17915) test shape: (5580, 17915)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9TfR1TK8PN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster = 1 #size of c\n",
        "h_len = 2000 #size of h\n",
        "batch_size = 32\n",
        "c_s = 1000\n",
        "epochs = 400\n",
        "eta = 0.00023"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaeTzYx0Kyzf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5274ab35-b520-40d7-ef4c-21a22e3bca06"
      },
      "source": [
        "#@title MVAE-BM model Build\n",
        "ep = 1e-20\n",
        "temp = 0.1\n",
        "\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None,trainX.shape[1]])\n",
        "batch = tf.placeholder(tf.int32, shape=())\n",
        "\n",
        "\n",
        "def encoder_h(x,name=\"classDecoder\"):\n",
        "\n",
        "        W1 = tf.Variable(tf.random.uniform([trainX.shape[1],1000],0.05,-0.05),name='W1',dtype=tf.float32)\n",
        "        b1 = tf.Variable(tf.zeros([1000]),name='bias1',dtype=tf.float32)\n",
        "\n",
        "        layer1 = tf.nn.sigmoid(tf.matmul(x,W1) + b1)\n",
        "\n",
        "        W2 = tf.Variable(tf.random.uniform([1000,750],0.05,-0.05),name='W2')\n",
        "        b2 = tf.Variable(tf.zeros([750]),name='bias2',dtype=tf.float32)\n",
        "\n",
        "        layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n",
        "\n",
        "        W3 = tf.Variable(tf.random.uniform([750,h_len],-0.05,0.05),name='WMu')\n",
        "        b3 = tf.Variable(tf.zeros([h_len]),name='biasMu',dtype=tf.float32)\n",
        "\n",
        "        W4 = tf.Variable(tf.random.uniform([750,h_len],0,0),name='WSigma')\n",
        "        b4 = tf.Variable(tf.zeros([h_len]),name='biasSigma',dtype=tf.float32)\n",
        "\n",
        "        mu = tf.matmul(layer2,W3) + b3\n",
        "        log_sigma = tf.matmul(layer2,W4) + b4\n",
        "\n",
        "        return mu,log_sigma\n",
        "\n",
        "def encoder_c(hs,name=\"C\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "    \n",
        "        W1 = tf.Variable(tf.random.uniform([trainX.shape[1],1000],-0.05,0.05),name='W1')\n",
        "        b1 = tf.Variable(tf.zeros([1000]),name='bias1',dtype=tf.float32)\n",
        "\n",
        "        \n",
        "        layer1 = tf.nn.tanh(tf.matmul(hs,W1) + b1)\n",
        "\n",
        "        W2 = tf.Variable(tf.random.uniform([1000,cluster],-0.05,0.05),name='W2')\n",
        "        b2 = tf.Variable(tf.zeros([cluster]),name='bias2',dtype=tf.float32)\n",
        "\n",
        "        layer2 = tf.nn.tanh(tf.matmul(layer1,W2) + b2)\n",
        "        return layer2\n",
        "\n",
        "\n",
        "def decoder(x,h,name='ClassEncoder'):\n",
        "\n",
        "     with tf.variable_scope(name):\n",
        "\n",
        "          R = tf.Variable(tf.random.uniform([h_len,trainX.shape[1]],-0.05,0.05),name='RWord')\n",
        "          b = tf.Variable(tf.zeros([trainX.shape[1]]),name='biasWord')\n",
        "\n",
        "          h_R = tf.nn.log_softmax(tf.matmul(h,R)+b)\n",
        "\n",
        "          un_probs = tf.reduce_sum(tf.multiply(h_R,x),axis=1)\n",
        "\n",
        "          return un_probs,R,h_R\n",
        "\n",
        "\n",
        "#h\n",
        "eps = tf.random_normal((batch,h_len), 0,1)\n",
        "mu,log_sigma = encoder_h(x)\n",
        "h = mu+tf.multiply(tf.exp(log_sigma),eps)\n",
        "\n",
        "#c \n",
        "eps2 = tf.random_uniform((batch,cluster), 0,1)\n",
        "eps2 = -tf.log(-tf.log(eps2+ep))\n",
        "C = encoder_c(x)\n",
        "C2 = C+eps2\n",
        "C2 = tf.nn.softmax(C2/temp,axis=1)\n",
        "\n",
        "\n",
        "# Doing mixture \n",
        "logits = []\n",
        "word_embedding = []\n",
        "topic = []\n",
        "\n",
        "for l in range(0,int(cluster)):\n",
        "\n",
        "    un_probs,R,h_R = decoder(x,h,name=\"Class_\"+str(l))\n",
        "    \n",
        "    logits.append(un_probs)\n",
        "    word_embedding.append(R)\n",
        "    topic.append(h_R)\n",
        "\n",
        "decoder_probs = tf.multiply(logits,tf.transpose(C2))\n",
        "decoder_probs = tf.reduce_sum(decoder_probs,axis=0)\n",
        "\n",
        "# Calcule ELBO\n",
        "\n",
        "kld_c = tf.reduce_sum(tf.nn.softmax(C,axis=1)*tf.log(tf.nn.softmax(C,axis=1)/(1.0/np.float(cluster))),axis=1)\n",
        "kld_h = -0.5 * tf.reduce_sum(1 - tf.square(mu) + 2 * log_sigma - tf.exp(2 * log_sigma), 1) \n",
        "\n",
        "ELBO = decoder_probs - kld_c - kld_h\n",
        "mean_ELBO = -tf.reduce_mean(ELBO)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=eta)\n",
        "\n",
        "step = optimizer.minimize(mean_ELBO)\n",
        "\n",
        "sess = tf.Session()\n",
        "print('Initializing...')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR0QnkPXQ-Ek",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b03c03-be2c-4379-97d8-e01569304717"
      },
      "source": [
        "#@title Train MVAE-BM\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "out = display(progress(0, 100), display_id=True)\n",
        "    \n",
        "\n",
        "\n",
        "keep_elbo_ppr = []\n",
        "\n",
        "for ll in range(0,epochs):\n",
        "\n",
        "    begin = 0\n",
        "    end = batch_size\n",
        "\n",
        "    while end <=trainX.shape[0]:\n",
        "\n",
        "        select = indices[begin:end]\n",
        "\n",
        "        feed_dict = {x:trainX[select].toarray().reshape(batch_size,trainX.shape[1]),batch:batch_size}\n",
        "        sess.run(step,feed_dict=feed_dict)\n",
        "\n",
        "        begin = end\n",
        "        end += batch_size\n",
        "            \n",
        "    #print (\"### EPOCH\",ll,\" #####\")\n",
        "    out.update(progress(ll, epochs))\n",
        "    feed_dict = {x:validX.toarray(),batch:validX.shape[0]}\n",
        "    elbo = sess.run(ELBO,feed_dict=feed_dict)\n",
        "    \n",
        "    # clean memory, importat for large dataset\n",
        "    feed_dict = {}\n",
        "    del feed_dict\n",
        "    feed_dict = 2\n",
        "\n",
        "    keep_elbo_ppr.append([np.mean(elbo),0])\n",
        "\n",
        "    # clean memory, importat for large dataset\n",
        "    #del ppr\n",
        "    #del val_matrix\n",
        "\n",
        "    val_matrix = 0\n",
        "    ppr = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='399'\n",
              "            max='400',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            399\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcCzCQHdCHne",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title MVAE-BM Encode Train and Test\n",
        "feed_dict = {x:trainX.toarray(),batch:trainX.shape[0]}\n",
        "c_train,h_train,mu_train = sess.run([C2,h,mu],feed_dict=feed_dict)\n",
        "\n",
        "feed_dict = {x:testX.toarray(),batch:testX.shape[0]}\n",
        "c_test,h_test,mu_test = sess.run([C2,h,mu],feed_dict=feed_dict)\n",
        "\n",
        "    \n",
        "feed_dict = {x:testX[0:1].toarray(),batch:1}\n",
        "words_embedding = sess.run(word_embedding)\n",
        "\n",
        "r_train = []\n",
        "cluster = np.argmax(c_train,axis=1)\n",
        "\n",
        "for sparse,c_cluster,mus in zip(trainX,cluster,mu_train):\n",
        "        \n",
        "        s = vectorizer.inverse_transform(sparse.toarray())[0]\n",
        "        aux = np.zeros(h_len)\n",
        "\n",
        "        for word in s:\n",
        "            aux += words_embedding[c_cluster][:,voc[word]]\n",
        "\n",
        "        if len(s) > 0:\n",
        "            rs = aux/np.float(len(s))\n",
        "\n",
        "        r_train.append(rs)\n",
        "\n",
        "r_test = []\n",
        "cluster = np.argmax(c_test,axis=1)\n",
        "\n",
        "for sparse,c_cluster,mus in zip(testX,cluster,mu_test):\n",
        "\n",
        "        s = vectorizer.inverse_transform(sparse.toarray())[0]\n",
        "        aux = np.zeros(h_len)\n",
        "\n",
        "        for word in s:\n",
        "            aux += words_embedding[c_cluster][:,voc[word]]\n",
        "\n",
        "        if len(s) > 0:\n",
        "\n",
        "            rs = aux/np.float(len(s))\n",
        " \n",
        "        r_test.append(rs)\n",
        " \n",
        "r_train = np.array(r_train)\n",
        "r_test = np.array(r_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2xzzkSFBnYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ba707036-d2e3-4524-ecb9-053d96f2bf19"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.metrics import accuracy_score\n",
        "    \n",
        "\n",
        "clf = LR().fit(mu_train,y_train);\n",
        "h_preds = clf.predict(mu_test);\n",
        "\n",
        "clf = LR().fit(r_train,y_train);\n",
        "r_preds = clf.predict(r_test);\n",
        "\n",
        "\n",
        "#clf = LR().fit(r_train2,y_train);\n",
        "#r_preds2 = clf.predict(r_test2);\n",
        "\n",
        "print ('Accuracy for H Representation:',accuracy_score(y_test,h_preds))\n",
        "print ('Accuracy for merge R Representation:',accuracy_score(y_test,r_preds))\n",
        "#print ('Accuracy for merge R Representation:',accuracy_score(y_test,r_preds2))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy for H Representation: 0.8125448028673835\n",
            "Accuracy for merge R Representation: 0.8130824372759856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
